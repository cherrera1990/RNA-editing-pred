# -*- coding: utf-8 -*-
"""TFM_LSTMBidiAttention_2CH_DataSelector_bla_d1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qalw4EGAtqJnufCt-T5-uQejMVXL7DWs
"""

!pip uninstall keras
!pip uninstall tensorflow
!pip install keras==3.2.1
!pip install tensorflow==2.15.1

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix, precision_score, cohen_kappa_score, classification_report
import seaborn as sns; sns.set()
from keras import backend as K

########################################
# PARAMS
########################################
fname      = 'bla_DL_d1_Pad_W50_12800GENES_BALANCED_1_1_CODED2CH.csv'
separator_char=';'
categories=[['A','G','C','T'],['s','d','h','i','b']]
padding=True
ptrain     = 0.7
trace_level = 1
model_name = 'TFM_LSTMBidiAttention_bla_d1'
num_lstm_units=256
n_epochs=20
batch_size=32
categories_size=20
random_seed=2022
save_train_val_test_datasets = True
load_train_val_test_datasets = False
########################################

"""# Deep neural network-based models for predicting mRNA editing

Bidirectional LSTM with Attention Layer

Possibility of loading customized train/test datasets

We can run this notebook in two environments:
+ Googlr Colab Pro (Cloud)
+ Jupyter Lab (Locally)
"""

#CHECK GOOGLE COLAB
GOOGLE_COLAB=False
if 'google.colab' in str(get_ipython()):
  GOOGLE_COLAB=True
  print('Running on CoLab')
else:
  print('Not running on CoLab')

"""Checking if GPU is available and GPU characteristics."""

#CHECK GPU
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print('GPU device not found')
else:
  print('Num GPUs Available: {}'.format(len(tf.config.list_physical_devices('GPU'))))
  print('Found GPU at: {}'.format(device_name))
  gpu_info = !nvidia-smi
  gpu_info = '\n'.join(gpu_info)
  if gpu_info.find('failed') >= 0:
     print('Not connected to a GPU')
  else:
     print(gpu_info)

if padding==True:
    categories_size=21

if GOOGLE_COLAB==False:
    data_dir = '../data/datasets/RAND1000/W50'
    models_dir = './SAVED_MODELS'
    train_val_test_sets_dir="./train_test_sets"
else:
    from google.colab import drive
    drive.mount('/content/drive')
    import sys
    #set the path to find RNAEd module
    sys.path.insert(0,'/content/drive/MyDrive/Colab_Notebooks')
    data_dir  = '/content/drive/MyDrive/Datasets_bla'
    models_dir= '/content/drive/MyDrive/Models_bla'
    train_val_test_sets_dir="/content/drive/MyDrive/Datasets_bla"

from rnaed import DataGenerator,DataEncoder
encoder=DataEncoder(categories,padding=padding,pad_char="*")
n_ch,cat=encoder.getChannels()
print("Number of channels: {} /categories: {}".format(n_ch,cat))
encoder.print_LUT_TupleToInteger()
categories_size=encoder.getNumCodes()
print("Number of codes or categories size: {}".format(categories_size))

#We can save best model and last model
best_model = 'best_model_'+model_name+'.h5'
last_model =  model_name+'.h5'
lastmodelfile = os.path.join(models_dir, last_model)
print("LAST MODEL FILE: {}".format(lastmodelfile))
bestmodelfile = os.path.join(models_dir, best_model)
print("BEST MODEL FILE: {}".format(bestmodelfile))
datafile = os.path.join(data_dir, fname)

"""Read data"""

print("DATASET: {}".format(datafile))
dataraw = pd.read_csv(datafile,sep=separator_char)
print("shape dataraw={}".format(dataraw.shape))

"""We will divide the data into: train, validation and test.
The three datastes should be well balanced.
"""

X=dataraw.iloc[:,3:]
Y=dataraw['EDITING']
print(X.head)

# Train and Validation+Test
x_train, x_val_test, y_train, y_val_test = train_test_split(X, Y, test_size=1-ptrain, random_state=random_seed)
#Validation and test: 0.5/0.5
x_val, x_test, y_val, y_test = train_test_split(x_val_test, y_val_test, test_size=0.5, random_state=random_seed)
#print("x_train is a {}".format(type(x_train)))
#print("x_val is a {}".format(type(x_val)))
#print("x_test is a {}".format(type(x_test)))
print("x_train:{} y_train:{}".format(x_train.shape,y_train.shape))
print("x_val: {} y_val: {}".format(x_val.shape,y_val.shape))
print("x_test: {} y_test: {}".format(x_test.shape,y_test.shape))
print("TRAIN LABELS:")
print(y_train.value_counts())
print("VALIDATION LABELS:")
print(y_val.value_counts())
print("TEST LABELS:")
print(y_test.value_counts())

"""# Save Train, Validation and Test datasets"""

#Save the train,val,test datasets
if save_train_val_test_datasets==True:
  #show input name dialog
  #Request a text for core name
  datasets_core_name=fname
  #Save the results into a file
  # X train
  x_train_csv_file  = datasets_core_name + "_x_train.csv"
  x_train.to_csv(os.path.join(train_val_test_sets_dir,x_train_csv_file),index=False,sep=separator_char)
  # Y train
  y_train_csv_file  = datasets_core_name + "_y_train.csv"
  y_train.to_csv(os.path.join(train_val_test_sets_dir,y_train_csv_file),index=False,sep=separator_char)
  # X val
  x_val_csv_file  = datasets_core_name + "_x_val.csv"
  x_val.to_csv(os.path.join(train_val_test_sets_dir,x_val_csv_file),index=False,sep=separator_char)
  # Y val
  y_val_csv_file  = datasets_core_name + "_y_val.csv"
  y_val.to_csv(os.path.join(train_val_test_sets_dir,y_val_csv_file),index=False,sep=separator_char)
  # X test
  x_test_csv_file  = datasets_core_name + "_x_test.csv"
  x_test.to_csv(os.path.join(train_val_test_sets_dir,x_test_csv_file),index=False,sep=separator_char)
  # Y test
  y_test_csv_file  = datasets_core_name + "_y_test.csv"
  y_test.to_csv(os.path.join(train_val_test_sets_dir,y_test_csv_file),index=False,sep=separator_char)

"""# Load Train, Validation and Test datasets"""

from ipyfilechooser import FileChooser

#Load the train,val,test datasets
if load_train_val_test_datasets==True:
  #show file selector dialog
  print("Select x_train file...")
  fc = FileChooser(train_val_test_sets_dir)
  display(fc)

if load_train_val_test_datasets==True:
  x_train = pd.read_csv(fc.selected,sep=separator_char)
  print(x_train.shape)
  print("Select y_train file...")
  fc = FileChooser(train_val_test_sets_dir)
  display(fc)

if load_train_val_test_datasets==True:
  y_train = pd.read_csv(fc.selected,sep=separator_char)
  print(y_train.shape)
  print("Select x_val file...")
  fc = FileChooser(train_val_test_sets_dir)
  display(fc)

if load_train_val_test_datasets==True:
  x_val = pd.read_csv(fc.selected,sep=separator_char)
  print(x_val.shape)
  print("Select y_val file...")
  fc = FileChooser(train_val_test_sets_dir)
  display(fc)

if load_train_val_test_datasets==True:
  y_val = pd.read_csv(fc.selected,sep=separator_char)
  print(y_val.shape)
  print("Select x_test file...")
  fc = FileChooser(train_val_test_sets_dir)
  display(fc)

if load_train_val_test_datasets==True:
  x_test = pd.read_csv(fc.selected,sep=separator_char)
  print(x_test.shape)
  print("Select y_test file...")
  fc = FileChooser(train_val_test_sets_dir)
  display(fc)

if load_train_val_test_datasets==True:
  y_test = pd.read_csv(fc.selected,sep=separator_char)
  print(y_test.shape)

"""We instantiate 3 DataGenerator objects that convert from integer to one-hot"""

#DataGenerator converts on the fly from raw data to one-hot
num_nucleosides = x_train.shape[1]
num_tokens = categories_size

training_generator   = DataGenerator(x_train, y_train, batch_size=batch_size, seq_size=num_nucleosides, categories_size=categories_size, shuffle=True)
validation_generator = DataGenerator(x_val,   y_val,   batch_size=batch_size, seq_size=num_nucleosides, categories_size=categories_size, shuffle=True)
test_generator       = DataGenerator(x_test,  y_test,  batch_size=1,seq_size=num_nucleosides, categories_size=categories_size, shuffle=False )

"""## Attention Layer"""

from keras.layers import *
from keras.models import *
from keras import backend as K

class attention(Layer):
    ''' heredamos de la clase layer '''
    def __init__(self):
        super(attention,self).__init__()

    #En el método build definimos los pesos
    def build(self, input_shape):
        # Define the shape of the weights and bias in this layer
        # The layer has just 1 lonely neuron

        # For convenience sake, add a couple of properties
        self.num_dim_pernucleoside = input_shape[-1] ## 512 dimension del vector de contexto.
        self.nucleosides_persequence = input_shape[-2]  ## 101 número de timesteps de la secuencia
        num_units = 1


        #Weights shape=(512,1), recordemos que la red neuronal capa oculta tiene un peso por dimensión.
        self.W=self.add_weight(name="att_weight", shape=(self.num_dim_pernucleoside,num_units),
                               initializer="normal")
        #bias  shape=(101,1), hay un bias por cada timestep, palabra o nucleótido.
        self.b=self.add_weight(name="att_bias", shape=(self.nucleosides_persequence,num_units),
                               initializer="zeros")
        super(attention,self).build(input_shape)

    #en el método call definimos la lógica
    #esta es la implementación clásica
    def call_classic(self, x):
        # x is the input tensor of 2 x num_units = 512 dimensions
        """ We multiply the inputs ‘x’ of shape (101,512) by the layer weights ‘w’
        of shape (512 , 1) and obtain a (101,1) values (future attention weights). We add the bias (101,1)
        and pass the output through any activation layer to form a neural network. So we now have 101 * 1 values and
        We take a softmax of these values. Softmax squashes these into values in the
        range between 0, and 1 whose sum is 1. These are the 101 attention weights.
        We multiply each attention weight by the respective nucleoside hidden state and sum up and we are done.
        We now have the ‘attention adjusted output’ state ready to be fed to the next dense layer for classification."""

        #e = K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)  # Necesario en algunos frameworks
        e = K.tanh(K.dot(x,self.W)+self.b) #Red neuronal densa de una neurona con una matriz de pesos W y un vector de bias
        a = K.softmax(e, axis=1) #hacemos softmax para normalizar y que los pesos sumen 1.
        # Multiplicamos cada hidden state por cada peso (101,512)*(101,1)=(101,512).
        # No es matricial, sino componente a componente
        output = x*a
        #Sumamos los 101 hidden states y se obtiene un hidden state suma de dimensión (101,512)
        #También se retornan los pesos a (101,1)
        return a, K.sum(output, axis=1)

    #esta implementación es más robusta
    def call(self, x):
        """ Es lo mismo de antes pero más robusto a distintos frameworks"""
        e = K.tanh(K.dot(x,self.W)+self.b) # dimensiones (101,512)(512,1)+(101,1)=(101,1)
        e = Flatten()(e) #(?,101)
        a = Activation('softmax')(e) #(?,101)
        # Repite 512 veces los 101 pesos (?,101) = (?,512,101)
        temp = RepeatVector(self.num_dim_pernucleoside)(a)
        #Permuta (?,512,101) por (?,101,512) para poder multiplicar componente a
        #componente después
        temp = Permute([2,1])(temp)
        # Multiplicamnos por cada peso todas las componentes de todos los hidden states
        # hidden state X=(101,512) pesos=temp=(101,512)
        # el truco aquí es que cada fila de pesos es una repetición de 512 pesos iguales.
        output = Multiply()([x,temp])
        #Finalmente sumamos todas las filas y obtenemos así el hidden state ponderado
        output = Lambda(lambda values: K.sum(values, axis=1))(output)
        #Retornamos los pesos de atención y la suma de los hidden states.
        return a, output

"""## Model defined from the scratch"""

#MODELOS REDES NEURONALES LSTM CON CAPAS DE ATENCIÓN
# Import the libraries required in this example:

num_nucleosides = x_train.shape[1]
num_tokens = categories_size
print("Number of units={}".format(num_lstm_units))
#Definimos la estructura con el Functional model
inputs = keras.Input(shape=(num_nucleosides,num_tokens), name="INPUTS")
lstm_out   = layers.Bidirectional(layers.LSTM(num_lstm_units,return_sequences=True), name="LSTM_BI")(inputs)
dropout= layers.Dropout(0.2)(lstm_out)

#===============================
#METHOD 1: using a custom layer:
#===============================
#La salida de la capa bidireccional será de 1 batch=None numero de secuencias,
#de 101 nucleosidos(timesteps=50+1+50) de 2 x número_unidades=2*256=512
#Es decir (None,101,512)
#---------uncomment here--------------------
#a ,att_d = attention()(dropout)
#-----------------------------
#La capa de atención debe retornarnos un peso para cada hidden state
#correspondiente a cada elemento de la secuencia o timestep
#es decir, nos debe dar un peso por cada nucleósido, es decir, nos debe dar (101,1)
#Como son pesos probabilisticos deben sumar 1.
#Pero realmente no queremos esos pesos, sino la suma de los 101 hidden states
#de dimensión 512 ponderados por esos pesos que es lo que conforma el verdadero contexto,
# y que tendrá dimensión (None,1,512).
#Esta salida es lo que se llama el "attention adjusted output state".
#Para ello, nuestra capa de atención debe entrenar (101,1) pesos, ya que debemos
#entrenar un peso por cada dimensión del espacio vectorial del contexto.
#Pero ojo, esos no son los pesos de la red neuronal de la capa de atención, porque si nuestra
#red neuronal de atención tuviera 101 pesos, serían posicionales y deben ser calculados por el
#valor semántico de los nucleótidos (si fueran palabras por su significado, no por su posición)

#Nuestra red neuronal de atención debe atender al "significado" de las palabras, es decir,
#a la codificación de cada nucleótido en el vector de estados
#de la LSTM que tiene dimensión 512. Por lo tanto el número de pesos de la  red de atención
# es de (512,1). Son los pesos de las 512 conexiones de la neurona de esta capa.

#Durante el entrenamiento multiplicaremos matricialmente todos los nucleótidos de la
#secuencia (101), con su codificación LSTMbidi(512), por los pesos de la capa de atención(512) y eso
#nos dará los 101 pesos de atención:
#(101,512) x (512,1) = (101,1) los pesos de atención
#(512,1) son los pesos de la capa.

#Ahora entendemos por qué los pesos de atención no son constantes
#sino que para cada secuencia cambian:
#En una red ya entrenada:
#  (101,512)[variable] x (512,1)[constante] = (101,1)[variable]

#===============================================================

#===============================
#METHOD 2: using regular layers:
#===============================

#---------uncomment here--------------------
e=Dense(1, activation='tanh')(dropout) #Capa de atención
e=Flatten()(e)
a=Activation('softmax')(e) #Softmax para que sumen 1 los pesos de atención.
temp=RepeatVector(2*num_lstm_units)(a)
temp=Permute([2, 1])(temp)
# multiply weight with LSTM layer o/p
output = Multiply()([dropout, temp])
# Get the attention adjusted output state by adding up
att = Lambda(lambda values: K.sum(values, axis=1))(output)
#-----------------------------

#===============================================================
outputs= layers.Dense(1, activation='sigmoid', name="OUTPUT")(att)
#Creamos el modelo
model_from_scratch  = keras.Model(inputs=inputs, outputs=outputs, name=model_name)

#Mostramos el modelo
print(model_from_scratch.summary())

#Compilation reset all weights
model_from_scratch.compile(optimizer=keras.optimizers.Adam(),
                           loss     =keras.losses.BinaryCrossentropy(),
                           metrics  =[keras.metrics.BinaryAccuracy()])

model=model_from_scratch

"""Recover last model, best model or generate a new model"""

#Load the trained model from disk
import os.path

best_model_accuracy =0
last_model_accuracy =0

if os.path.exists(bestmodelfile):
    model = tf.keras.models.load_model(bestmodelfile)
    print("Testing best model file: {}".format(bestmodelfile))
    results = model.evaluate(validation_generator,verbose=1)
    best_model_accuracy = results[1]
    print("best model accuracy: {}".format(best_model_accuracy))

if os.path.exists(lastmodelfile):
    model = tf.keras.models.load_model(lastmodelfile)
    print("Testing last complete model file: {}".format(lastmodelfile))
    results = model.evaluate(validation_generator,verbose=1)
    last_model_accuracy = results[1]
    print("last model accuracy: {}".format(last_model_accuracy))

if best_model_accuracy > last_model_accuracy:
   model = tf.keras.models.load_model(bestmodelfile)
   print("Selected best model saved: {}".format(bestmodelfile))
else:
  if last_model_accuracy > best_model_accuracy:
     print("Selected last model saved: {}".format(lastmodelfile))
  else:
     print("Selected new untrained model")


# Show the model architecture
model.summary()

keras.utils.plot_model(model, show_shapes=True)

from keras.callbacks import ModelCheckpoint,EarlyStopping

checkpoint = ModelCheckpoint(bestmodelfile,
                             monitor='val_loss',
                             verbose=1,
                             save_best_only=True,
                             mode='auto',
                             save_freq='epoch')

callback=EarlyStopping(monitor='val_loss',
                       min_delta=0,
                       patience=3,
                       verbose=1,
                       mode='auto',
                       baseline=None,
                       restore_best_weights=True)

with tf.device(device_name):#Use GPU if available
    history=model.fit(training_generator,epochs=n_epochs,
                      validation_data=validation_generator,
                      callbacks=[checkpoint,callback],verbose=trace_level)

#We always save the last model.
#Last model can be triggered by early stopping
n_epochs_trained = len(history.history['loss'])
print("Trained epochs: {} of {}".format(n_epochs_trained,n_epochs))
model.save(lastmodelfile)

#PLOT ACCURACY AND LOSS

acc = history.history['binary_accuracy']
val_acc = history.history['val_binary_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()

"""We recover the best model saved."""

if os.path.exists(bestmodelfile):
    model = tf.keras.models.load_model(bestmodelfile)

"""We evaluate the best model on validation data."""

#EVALUATE VALIDATION DATASET
print("Evaluate models on validation data")
results_model = model.evaluate(validation_generator,verbose=trace_level)
print ('Accuracy on validation data:',round(results_model[1],3))
print ('Loss on validation data:',round(results_model[0],3))

"""We make predictions on test data, never used during training nor validation"""

#PREDICTIONS
print("Make predictions on test data")
model_prediction = model.predict(test_generator,verbose=trace_level)
model_prediction_binary = (model_prediction > 0.5).astype("int32")

print(x_test.shape)
print(y_test.shape)
print(model_prediction.shape)
print(model_prediction_binary.shape)

#METRICS QUALITY CLASSIFICATION
print("MODEL "+model_name+"\n---------------------------------")
kappa = cohen_kappa_score(y_test, model_prediction_binary)
print ('Kappa:',round(kappa,3))
print(classification_report(y_test, model_prediction_binary))

from sklearn.metrics import confusion_matrix, precision_score, cohen_kappa_score, classification_report
import seaborn as sns; sns.set()
# PLOT CONFUSION MATRIX
# Tutorial: https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/
plt.figure()

cm = confusion_matrix(y_test,model_prediction_binary)

group_names = ['TN','FP','FN','TP']
group_counts = ["{0:0.0f}".format(value) for value in cm.flatten()]
group_percentages = ["{0:.2%}".format(value) for value in cm.flatten()/np.sum(cm)]

labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

ax = sns.heatmap(cm, annot=labels, fmt='', cmap='Blues')

ax.set_title('RNA-Editing Confusion Matrix \n\n');
ax.set_xlabel('\nPredicted Values '+ model_name)
ax.set_ylabel('Actual Values ');
## Ticket labels - List must be in alphabetical order
#ax.xaxis.set_ticklabels(['False','True'])
#ax.yaxis.set_ticklabels(['False','True'])
plt.show()

from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score

def plot_roc_curve(fpr, tpr):
    plt.plot(fpr, tpr, color="orange", label="ROC")
    plt.plot([0, 1], [0, 1], color="darkblue", linestyle="--")
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend()
    plt.show()

auc = roc_auc_score(y_test, model_prediction)
print('AUC: %.2f' % auc)
fpr, tpr, thresholds = roc_curve(y_test, model_prediction)
plot_roc_curve(fpr, tpr)